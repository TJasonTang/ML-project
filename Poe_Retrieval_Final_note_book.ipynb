{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfakDXFX51aT",
        "outputId": "adc4d21d-97f3-45ef-c66c-02101888adee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.0/223.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.0/218.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.0/218.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-iam 2.12.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-resource-manager 1.10.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.3 which is incompatible.\n",
            "tensorboard 2.13.0 requires protobuf>=3.19.6, but you have protobuf 3.19.3 which is incompatible.\n",
            "tensorflow 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.3 which is incompatible.\n",
            "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.3 which is incompatible.\n",
            "tensorflow-hub 0.15.0 requires protobuf>=3.19.6, but you have protobuf 3.19.3 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  openai==0.27.7 \\\n",
        "  \"pinecone-client[grpc]\"==2.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDkwU5QE6eRI",
        "outputId": "955e620e-0e83-4edf-dd7f-8bc5af8b9c16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import openai, pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1uZ4PUi6Gdj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PINECONE_ENVIRONMENT']='gcp-starter'\n",
        "os.environ['PINECONE_API_KEY']='bcdfe7e2-2bb7-438a-9503-1302f8cf7a88'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxmKNmkX6Xkd"
      },
      "outputs": [],
      "source": [
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
        "# find your environment next to the api key in pinecone console\n",
        "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
        "\n",
        "pinecone.init(api_key=api_key, environment=env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoVl-MvK6tg7"
      },
      "outputs": [],
      "source": [
        "index_name = 'my-poe-knowledgebase'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhzrV7AE6q0t",
        "outputId": "ec6162d7-da3c-4f40-85f2-e4a94d7ea56f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.02602,\n",
              " 'namespaces': {'': {'vector_count': 2602}},\n",
              " 'total_vector_count': 2602}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = pinecone.GRPCIndex(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IjBjpl56-QE"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCYOaKAS6_MO"
      },
      "outputs": [],
      "source": [
        "openai.api_key = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO3xn7Cd7Mpy"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_query(query,embed_model,k=5):\n",
        "  query_embedding = openai.Embedding.create(\n",
        "      input=[query],\n",
        "      engine=embed_model)\n",
        "  xq = query_embedding['data'][0]['embedding']\n",
        "\n",
        "  res = index.query(xq, top_k=k,include_metadata=True)\n",
        "\n",
        "  contexts =[item['metadata']['text'] for item in res['matches']]\n",
        "\n",
        "  return \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "DyoOkV0oLARP",
        "outputId": "6e9ebe27-1ea4-4de5-d440-902b16503af7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The next day’s paper had these additional particulars.\\n\\n      “_The Tragedy in the Rue Morgue._—Many individuals have been\\n      examined in relation to this most extraordinary and frightful\\n      affair” [The word ‘affaire’ has not yet, in France, that levity\\n      of import which it conveys with us], “but nothing whatever has\\n      transpired to throw light upon it. We give below all the material\\n      testimony elicited.\\n\\n      “_Pauline Dubourg_, laundress, deposes that she has known both\\n      the deceased for three years, having washed for them during that\\n      period. The old lady and her daughter seemed on good terms—very\\n      affectionate towards each other. They were excellent pay. Could\\n      not speak in regard to their mode or means of living. Believed\\n      that Madame L. told fortunes for a living. Was reputed to have\\n      money put by. Never met any persons in the house when she called\\n      for the clothes or took them home. Was sure that they had no\\n      servant in employ. There appeared to be no furniture in any part\\n      of the building except in the fourth story.\\n\\n---\\n\\nWhen, in an article entitled “The Murders in the Rue Morgue,” I\\n      endeavored, about a year ago, to depict some very remarkable\\n      features in the mental character of my friend, the Chevalier C.\\n      Auguste Dupin, it did not occur to me that I should ever resume\\n      the subject. This depicting of character constituted my design;\\n      and this design was thoroughly fulfilled in the wild train of\\n      circumstances brought to instance Dupin’s idiosyncrasy. I might\\n      have adduced other examples, but I should have proven no more.\\n      Late events, however, in their surprising development, have\\n      startled me into some farther details, which will carry with them\\n      the air of extorted confession. Hearing what I have lately heard,\\n      it would be indeed strange should I remain silent in regard to\\n      what I both heard and saw so long ago.\\n\\n      Upon the winding up of the tragedy involved in the deaths of\\n      Madame L’Espanaye and her daughter, the Chevalier dismissed the\\n      affair at once from his attention, and relapsed into his old\\n      habits of moody reverie. Prone, at all times, to abstraction, I\\n      readily fell in with his humor; and, continuing to occupy our\\n      chambers in the Faubourg Saint Germain, we gave the Future to the\\n      winds, and slumbered tranquilly in the Present, weaving the dull\\n      world around us into dreams.\\n\\n---\\n\\nVOLUME 2.\\n\\n[Redactor’s Note—Some endnotes are by Poe and some were added by\\nGriswold. In this volume the notes are at the end.]\\n\\n\\n\\n\\nTHE PURLOINED LETTER\\n\\n\\nNil sapientiæ odiosius acumine nimio.—_Seneca_.\\n\\n      At Paris, just after dark one gusty evening in the autumn of 18-,\\n      I was enjoying the twofold luxury of meditation and a meerschaum,\\n      in company with my friend C. Auguste Dupin, in his little back\\n      library, or book-closet, _au troisième_, No. 33, _Rue Dunôt,\\n      Faubourg St. Germain_. For one hour at least we had maintained a\\n      profound silence; while each, to any casual observer, might have\\n      seemed intently and exclusively occupied with the curling eddies\\n      of smoke that oppressed the atmosphere of the chamber. For\\n      myself, however, I was mentally discussing certain topics which\\n      had formed matter for conversation between us at an earlier\\n      period of the evening; I mean the affair of the Rue Morgue, and\\n      the mystery attending the murder of Marie Rogêt. I looked upon\\n      it, therefore, as something of a coincidence, when the door of\\n      our apartment was thrown open and admitted our old acquaintance,\\n      Monsieur G——, the Prefect of the Parisian police.\\n\\n---\\n\\n“_Extraordinary Murders_.—This morning, about three o’clock, the\\n      inhabitants of the Quartier St. Roch were aroused from sleep by a\\n      succession of terrific shrieks, issuing, apparently, from the\\n      fourth story of a house in the Rue Morgue, known to be in the\\n      sole occupancy of one Madame L’Espanaye, and her daughter,\\n      Mademoiselle Camille L’Espanaye. After some delay, occasioned by\\n      a fruitless attempt to procure admission in the usual manner, the\\n      gateway was broken in with a crowbar, and eight or ten of the\\n      neighbors entered accompanied by two _gendarmes_. By this time\\n      the cries had ceased; but, as the party rushed up the first\\n      flight of stairs, two or more rough voices in angry contention\\n      were distinguished and seemed to proceed from the upper part of\\n      the house. As the second landing was reached, these sounds, also,\\n      had ceased and everything remained perfectly quiet. The party\\n      spread themselves and hurried from room to room. Upon arriving at\\n      a large back chamber in the fourth story, (the door of which,\\n      being found locked, with the key inside, was forced open,) a\\n      spectacle presented itself which struck every one present not\\n      less with horror than with astonishment.\\n\\n---\\n\\n“As for these murders, let us enter into some examinations for\\n      ourselves, before we make up an opinion respecting them. An\\n      inquiry will afford us amusement,” [I thought this an odd term,\\n      so applied, but said nothing] “and, besides, Le Bon once rendered\\n      me a service for which I am not ungrateful. We will go and see\\n      the premises with our own eyes. I know G——, the Prefect of\\n      Police, and shall have no difficulty in obtaining the necessary\\n      permission.”\\n\\n      The permission was obtained, and we proceeded at once to the Rue\\n      Morgue. This is one of those miserable thoroughfares which\\n      intervene between the Rue Richelieu and the Rue St. Roch. It was\\n      late in the afternoon when we reached it, as this quarter is at a\\n      great distance from that in which we resided. The house was\\n      readily found; for there were still many persons gazing up at the\\n      closed shutters, with an objectless curiosity, from the opposite\\n      side of the way. It was an ordinary Parisian house, with a\\n      gateway, on one side of which was a glazed watch-box, with a\\n      sliding panel in the window, indicating a _loge de concierge._\\n      Before going in we walked up the street, turned down an alley,\\n      and then, again turning, passed in the rear of the\\n      building—Dupin, meanwhile examining the whole neighborhood, as\\n      well as the house, with a minuteness of attention for which I\\n      could see no possible object.\\n\\n-----\\n\\nWhat is the state of the Rue Morgue?'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_augmented_query(\"What is the state of the Rue Morgue?\",\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG1Nd7cn7kD5"
      },
      "outputs": [],
      "source": [
        "def llm_answer(llmmodel,primer,augmented_query):\n",
        "\n",
        "  comp = openai.ChatCompletion.create(\n",
        "    model=llmmodel,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ])\n",
        "  display(Markdown(comp['choices'][0]['message']['content']))\n",
        "\n",
        "  return comp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1Cbz6Ni7oRy"
      },
      "outputs": [],
      "source": [
        "def rag_response(query):\n",
        "  embed_model=\"text-embedding-ada-002\"\n",
        "  primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
        "  user questions based on the information provided by the user above\n",
        "  each question. If the information can not be found in the information\n",
        "  provided by the user you truthfully say \"I don't know\".\n",
        "  \"\"\"\n",
        "  model=\"gpt-4\"\n",
        "\n",
        "  display(Markdown(query))\n",
        "  llm_answer(model,primer,generate_augmented_query(query,embed_model))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "n1VNI3Uc7vhR",
        "outputId": "edaa999f-d94c-4bbe-968c-2ba3af2872e2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "What is in the cask of Amontillado?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "The text does not specify what is in the cask of Amontillado.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rag_response(\"What is in the cask of Amontillado?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BGOsFfFm735z",
        "outputId": "ef360691-e269-4a68-fb41-cce001b06efd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "who is buryed in the House of Usher?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "The person buried in the House of Usher is the twin sister of Roderick Usher. Her death and burial are described in the text, but her name is not provided in the provided information.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rag_response('who is buryed in the House of Usher?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8KJjdD_h976I",
        "outputId": "8e8f7d28-8870-4e1d-e312-243fd9e0f471"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Why does the raven tormnent the author?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "The raven in the poem represents a messenger of the underworld, symbolizing the eternal remembrance of the narrator's lost love Lenore. The word \"Nevermore\" repeatedly uttered by the raven causes the narrator increasing distress as it reminds him of his loss and the fact that he will \"nevermore\" be reunited with Lenore. The torment is not caused by the physical presence of the raven itself, but rather the psychological torment the narrator experiences from his memories and the realization of his loss.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rag_response(\"Why does the raven tormnent the author?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHHkeoKB-Fpf"
      },
      "outputs": [],
      "source": [
        "rag_response(\"List all of the works of Edgar Allan Poe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvgzkAaNt7Oc"
      },
      "outputs": [],
      "source": [
        "rag_response(\"Tell me about 'Murders in the Rue Morgue'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8FUovHtu3q3"
      },
      "outputs": [],
      "source": [
        "rag_response(\"compare and contrast  'Murders in the Rue Morgue' and ' A cask of Amontillado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4BhkNCLu9Wm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}